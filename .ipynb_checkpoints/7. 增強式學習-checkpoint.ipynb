{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本課程由成大與台南二中共同開發的人工智慧高中教材改編"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9qqu6j7gQ4e"
   },
   "source": [
    "# 實作範例 5 - 增強式學習 Reinforcement Learning\n",
    "\n",
    "歡迎大家來到增強式學習（又稱強化學習）的世界！增強式學習不同於前面章節介紹過的，利用已知資料來訓練機器；而是**透過「環境」對於機器的「行動」所給予的「獎勵或懲罰」來影響及訓練機器的決策**，讓機器表現愈來愈好的一種學習方式。就像我們在成長的過程中，做錯事了會被懲罰；而做得很棒會被獎勵一樣，增強式學習正是利用這樣的概念來訓練機器的。\n",
    "\n",
    "本章實作範例將藉由 [**OpenAI Gym**](https://gym.openai.com) 套件，來模擬增強式學習中的**環境**，讓我們在其上實作及驗證我們的演算法。OpenAI Gym 是一套提供許多「環境」供增強式學習演算法實作與驗證的平台。如果不依靠 OpenAI Gym，我們除了實作演算法 (如課堂介紹的 [Q-Learning](https://zh.wikipedia.org/zh-tw/Q學習)) 之外，我們也必須要自行設計一套「環境」給演算法跑，包括：根據演算法的行動給予獎勵或懲罰、判斷某個特定目標是不是已經完成或失敗了、提供演算法目前的狀態﹍等。由於自行設計與開發環境包含了上面許多與演算法本身無關的雜事，為了讓使用者**有一個共同的測試環境與專注於開發與驗證增強式學習的演算法**，OpenAI Gym 出現了，它幫你把一系列的雜事都包了！讓你能專心搞定演算法，而不用為了環境的事煩心。\n",
    "\n",
    "那什麼是「環境」呢？聽請來很抽象，但概念其實很簡單。假設我們今天要讓機器學習玩[打磚塊](https://www.google.com.tw/search?q=打磚塊&tbm=isch)，這個遊戲本身就是一個「環境」。而此時機器能做的「行動」，也就是玩家能做的事情，就是將遊戲畫面下方的板子左右移動。而根據這些行動，環境需要判斷：球是不是掉下去了、球是不是有彈起來、球有沒有打到磚塊、打到多少磚塊、玩家是不是將所有的磚塊都消去了 (成功！)、是不是所有的生命都用完了 (失敗！)﹍等等。而根據這些機器行動後的結果，環境需要給機器回饋，例如碰到磚塊的話就給正向回饋、球掉下去就給負面回饋等。這些就是一個「環境」需要做的事，也就是 OpenAI Gym 幫我們做的事。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lvn8Jgv9lOs2"
   },
   "source": [
    "## OpenAI Gym 環境設定\n",
    "\n",
    "### 安裝 OpenAI Gym\n",
    "\n",
    "OpenAI Gym 與之前介紹的其他 Python 套件相同，使用 `pip` 安裝即可（或是在 Anaconda 上使用 `conda` 命令）：\n",
    "\n",
    "```\n",
    "pip install gym\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Yo4RwLNlOs3"
   },
   "source": [
    "### &star; 使用遠端伺服器或 Google Colab 上之 Jupyter Notebook 需進行的額外設定 &star;\n",
    "\n",
    "我們利用 OpenAI Gym 時會將環境的狀態顯示在螢幕上，但在如 [Google Colab](https://medium.com/@ericsk/9f92c7bb1f50) 等*遠端*環境中，由於我們的螢幕並沒有真的連接到遠端伺服器，所以無法直接看到 OpenAI Gym 所秀出來的環境狀態。因此需要額外安裝一些套件與執行額外設定，讓我們能看到遠端執行的 OpenAI Gym 的結果。\n",
    "\n",
    "若同學是在**遠端伺服器或 Google Colab 上之 Jupyter Notebook** 執行本範例，請執行底下的指令與程式碼安裝必要套件與進行設定："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gIwYTjZg52W"
   },
   "outputs": [],
   "source": [
    "## 若使用遠端伺服器或 Google Colab 上之 Jupyter Notebook 者，請執行本區塊指令 ##\n",
    "!apt-get install -y xvfb x11-utils > /dev/null\n",
    "!pip install gym pyvirtualdisplay PyOpenGL > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mt7UrrTDlOs8"
   },
   "source": [
    "如果以上的執行結果沒有任何輸出，就表示成功了。接著請執行以下 Python 程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wa46qsGDlOs9"
   },
   "outputs": [],
   "source": [
    "## 若使用遠端伺服器或 Google Colab 上之 Jupyter Notebook 者，請執行本區塊程式 ##\n",
    "import gym # 載入 OpenAI Gym\n",
    "import matplotlib.pyplot as plt # 載入 matplotlib，幫我們顯示遠端 OpenAI Gym 執行的結果\n",
    "from IPython import display as ipythondisplay # 控制 IPython 環境中的顯示\n",
    "from pyvirtualdisplay import Display # 虛擬顯示裝置\n",
    "\n",
    "# 產生一個 Display 物件，並且設定在 Google Colab 上的顯示參數\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "\n",
    "# Making a monkey patch to maintain the consistency of APIs in both settings\n",
    "gym.Wrapper.orig_render = gym.Wrapper.render\n",
    "gym.Wrapper.orig_close  = gym.Wrapper.close\n",
    "\n",
    "def new_render(self):\n",
    "    self.vscreen = self.orig_render(mode='rgb_array')\n",
    "    plt.imshow(self.vscreen)\n",
    "    ipythondisplay.clear_output(wait=True) \n",
    "    ipythondisplay.display(plt.gcf())\n",
    "\n",
    "def new_close(self):\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    self.orig_close()\n",
    "\n",
    "gym.Wrapper.render = new_render\n",
    "gym.Wrapper.close  = new_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EEIF08yIkG1L"
   },
   "source": [
    "### 載入套件\n",
    "\n",
    "接著載入需要的 Python 套件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b1nQ3BrXlOtB"
   },
   "outputs": [],
   "source": [
    "import gym # 載入 OpenAI Gym\n",
    "import math # 載入數學運算模組\n",
    "import numpy as np # 載入 NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LWubcq_lOtF"
   },
   "source": [
    "設定完了之後，我們就可以準備進入 OepnAI Gym 的世界了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DK43viqDoE-A"
   },
   "source": [
    "## 初探 OepnAI Gym\n",
    "\n",
    "我們從一個最簡單的環境：「**CartPole-v0** ([官網介紹](https://gym.openai.com/envs/CartPole-v0/), [詳細資訊](https://github.com/openai/gym/wiki/CartPole-v0))」開始，這是在增強式學習中其中一個最知名的問題。在這個環境中，有一根棒子垂直放在一個會移動的平台上，而我們必須要左右移動這個平台，維持棒子的平衡。同學們可以自行想像一下*將一根掃把棍頂在頭上，然後想辦法左右移動讓這個掃把棍不要掉下來*﹍﹍這個環境差不多就是在做這件事。\n",
    "\n",
    "OpenAI Gym 的[官方網站](https://gym.openai.com/envs)上有列出它所提供的所有環境 (在 GitHub 的 wiki 上也有一張[列表](https://github.com/openai/gym/wiki/Table-of-environments)，詳細比較了各個不同環境的參數)。\n",
    "\n",
    "我們先看一下以下的程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1562304752007,
     "user": {
      "displayName": "Jiangsir jiang",
      "photoUrl": "https://lh3.googleusercontent.com/-IRgxJH2IITY/AAAAAAAAAAI/AAAAAAAAEPs/XqkMV_OB0lM/s64/photo.jpg",
      "userId": "14587696984096323544"
     },
     "user_tz": -480
    },
    "id": "kIrncGwOlOtG",
    "outputId": "4181ae15-b1d8-4bdc-925f-4a0f853892b3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8JLUKQNekjplwEtSpwx4SerU/wNMvNYp6wIzpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0') # 創造一個 CartPole-v0 環境\n",
    "env.reset() # 初始化/重設環境狀態\n",
    "for t in range(20): # 讓它跑 20 個間點 (行動 20 次)\n",
    "    env.step(env.action_space.sample()) # 隨機做一個行動\n",
    "    print(\"現在是第 {} 個時間點\".format(t+1))\n",
    "    env.render() # 顯示目前的環境狀態\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Jj9-TEhlOtL"
   },
   "source": [
    "執行後，同學們應該可以看見一根棒子與平台在移動，這就是最簡單的 OpenAI Gym 程式：\n",
    "\n",
    "1. 程式一開始，我們先使用 `gym.make()` 來創造一個**環境**，程式碼中我們填上 `'CartPole-v0'` 代表使用「CartPole-v0」這個環境。如果填上不同的值，代表創造其他的環境 (例如 `'MountainCar-v0'`, `'Acrobot-v1'`, `'Pendulum-v0'` 等，同學們可以自行嘗試。*但 OpenAI Gym 網站上的部份環境可能需要額外安裝其他套件*)。\n",
    "2. 接著呼叫 `reset()` 來初始化環境的**狀態**。\n",
    "3. 然後設定一個跑 20 次的迴圈，裡面我們呼叫了 `render()`，代表把目前的狀態顯示在畫面上；`step()` 則代表**做出行動**，我們這邊先讓電腦在每一個時間點都隨機選一個行動（細節稍後講解）。\n",
    "4. 最後等迴圈跑完之後，我們呼叫 `close()` 將這個環境給結束。\n",
    "\n",
    "### 與環境互動\n",
    "\n",
    "增強式學習中強調**「機器 (agent)」與「環境 (environment)」的互動**，以上的步驟簡單地呈現了機器與環境的互動過程。但我們知道，當機器在環境中做出「行動 (action)」後，環境會給予對於該行動之「獎勵 (reward)」與行動完後環境的「狀態 (observation/state)」：\n",
    "\n",
    "`[機器 (agent)]` &rarr;&rarr;&rarr; `做出行動 (action)` &rarr;&rarr;&rarr; `[環境 (environment)]`\n",
    "\n",
    "`[機器 (agent)]` &larr;&larr;&larr; `獲得獎勵 (reward) & 行動後狀態 (observation/state)` &larr;&larr;&larr; `[環境 (environment)]`\n",
    "\n",
    "有了以上概念後，我們稍微擴充一下之前的程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "5j4iD6zwoJ1V",
    "outputId": "743307fc-94af-427e-cbb5-2bea402f172c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "環境已達終止條件，總共獲得獎勵： 29.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0') # 創造一個 CartPole-v0 環境\n",
    "observation = env.reset() # 初始化/重設環境狀態，該函式會回傳環境的初始狀態\n",
    "\n",
    "total_rewards = 0.0\n",
    "for t in range(50): # 至多行動 50 次\n",
    "    action = env.action_space.sample() # 隨機選一個行動\n",
    "    observation, reward, done, info = env.step(action) # 執行行動，並接收行動後的獎勵與環境狀態\n",
    "    total_rewards += reward\n",
    "    \n",
    "    print(\"現在是第 {} 個時間點，獲得獎勵 {}，累計獎勵 {}，是否達終止條件？ {}\".format(t+1, reward, total_rewards, done))\n",
    "    env.render() # 顯示目前的環境狀態\n",
    "    \n",
    "    if done: # 若達終止條件，則結束執行\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"環境已達終止條件，總共獲得獎勵：\", total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TWhlp3jfLE-e"
   },
   "source": [
    "首先注意到，在 `observation, reward, done, info = env.step(action)` 這行程式中，`env.step()` 函數前多了四個變數，這些變數正是我們在行動後，環境回傳給我們的**行動獎勵**與**行動後狀態**，這四個變數依照順序分別為：\n",
    " * `observation`: (執行該行動後) 環境的狀態\n",
    " * `reward`: (執行該行動後) 所獲得的獎勵\n",
    " * `done`: (執行該行動後) 是否已達終止條件\n",
    " * `info`: Debug (除錯) 用的資訊\n",
    "\n",
    "`observation` 與 `reward` 會根據各環境不同而有自己的定義，而 `done` 代表的則是該環境是否已達到「**終止條件**」。什麼是終止條件呢？以前面提過的打磚塊遊戲為例，就是*造成遊戲結束的條件*。當遊戲結束時，可能是玩家*成功地消去了所有磚塊（破關）*，或者是*所有的球都掉下去了（game over）*。當回傳的 `done` 被設為 `True` 時，代表這個環境可能已達成 (或已不可能達成) 某種目標，提示機器不需要再進行下去了。\n",
    "\n",
    "我們所設計的演算法，最終的目標就是要**在達到終止條件前，獲得最多的獎勵**。以範例中的 CartPole-v0 環境為例，只要每多存活一個時間點，就會獲得 1 點獎勵。所以我們的目標就是要控制平台左右移動，讓棒子盡量保持平衡而不掉下來。我們目前在選擇行動的時候是隨便選，這樣得到的分數只有少少的十幾二十幾分，後面我們要以能夠突破這個分數為目標前進！\n",
    "\n",
    "至於 `info` 是一個 `dict` 物件，裡面有方便程式設計師除錯的資訊，內容也會根據環境的不同而有所改變。（註：[官方的教學](https://gym.openai.com/docs/)上面有特別提到，這個變數中可能會含有一些「不應該」被觀察到的資訊，提供這些資訊僅是方便除錯工作，在正式對演算法做測試與評估時，是嚴格禁止讀取裡面的資訊作為演算法之判斷依據的。）\n",
    "\n",
    "了解回傳值的內容後可以明白，**在 OpenAI Gym 中，我們就是透過 `step()` 來讓電腦與環境進行互動**的。往後我們的程式邏輯，可以依照底下幾個步驟進行：\n",
    " 1. 初始化環境 (`reset()`) ，得到環境的**初始狀態** (`observation`)\n",
    " 2. 根據最初的環境選擇一個**行動** (`step()`)\n",
    " 3. 從 `step()` 的結果中獲得該行動後的**環境狀態**與**獎勵**\n",
    " 4. 根據上一次行動後的環境狀態與獲得獎勵，讓機器學習，然後選擇一個**最佳行動** (`step()`)\n",
    " 5. 回到第 3. 步，直到環境達到**終止條件**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzlTX1aOlOtR"
   },
   "source": [
    "### OpenAI Gym 中的 Space\n",
    "\n",
    "（註：這一小節中提到的關於 CartPole-v0 環境的細節，在 [OpenAI Gym 的 wiki 上](https://github.com/openai/gym/wiki/CartPole-v0)有更詳細的資料與解說，老師鼓勵同學們可以直接閱讀 wiki 上的資料，會比較有幫助喔！）\n",
    "\n",
    "#### Observation Space (狀態空間)\n",
    "\n",
    "了解在 OpenAI Gym 上機器如何與環境互動後，我們接下來看看機器回傳給我們的 observation 裡面有什麼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRwaZnMalOtS",
    "outputId": "3aa4fbbd-23dd-4974-d440-f365d7051e6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.46299119e-02,  1.28613260e-02, -4.83290144e-05, -4.59956610e-02])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pp4IzrlplOtV"
   },
   "source": [
    "可以看到裡面有 4 個值，依照 [wiki 上的說明](https://github.com/openai/gym/wiki/CartPole-v0)，這四個值分別是：\n",
    " * 平台的位置 (有效值：-2.4 ~ 2.4)\n",
    " * 平台的移動速度 (有效值：-&infin; ~ +&infin;)\n",
    " * 棒子的傾斜角度 (有效值：-41.8° ~ 41.8°)\n",
    " * 棒子的末端速度 (有效值：-&infin; ~ +&infin;)\n",
    "\n",
    "我們把這些狀態上的*變數組合*稱為「**observation space**」，用來**描述環境的格式和範圍**。每個環境狀態的格式可以透過 `observation_space` 屬性取得："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qY6DP2XxlOtW",
    "outputId": "a47099af-392d-46a2-9410-68eead36e51f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B04weX4PlOta"
   },
   "source": [
    "我們可以看到在 CartPole-v0 中，用來描述環境的格式是 `Box(4,)`，`Box` 類型的物件，用來表示一個 $n$ 維的*盒子*，所以在上面我們會發現 `observation` 是一個長度為 4 的陣列，而陣列中的每個變數都有上下界（可以利用 `observation_space.high` 與 `observation_space.low` 查看），看起來就像一個盒子一般。\n",
    "\n",
    "#### Action Space (行動空間)\n",
    "\n",
    "除了 observation space 外，OpenAI Gym 還有另一個空間，稱為「**action space**」，用來描述有效的動作範圍。每個環境中 action space 的格式可以透過 `action_space` 屬性取得："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-v1T4KQlOtb",
    "outputId": "a5a17cd5-c61d-4723-a95b-3dad1f4f9bb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pq05jjTQlOte"
   },
   "source": [
    "`Discrete(n)` 類別的物件是一個 $\\{0, 1, \\dots, n-1\\}$ 的*非負整數*集合。在 CartPole-v0 例子中，action space 的格式為 `Discrete(2)`，代表該環境中有效的行動為 $\\{0, 1\\}$，分別代表將平台推往左邊與右邊。\n",
    "\n",
    "`Discrete` 物件有一個方法叫 `sample()`，其意義是隨機從 action space 中挑選一個動作。我們在前面的程式中，正是透過這個方法來讓電腦幫我們隨機挑選行動的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqmoV9UclOtf",
    "outputId": "1afa8d90-f909-434f-e05f-d969f6e0968a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPYqQCJJlOtj"
   },
   "source": [
    "同學們可以重複執行上一行程式，看看是不是會隨機出現不同數字呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0AqRmXszeHp"
   },
   "source": [
    "## 改進策略之 1：先來惡搞一下\n",
    "\n",
    "了解 OpenAI Gym 的基本概念後，我們終於可以來開始改進我們的策略了！\n",
    "\n",
    "但在那之前，我們先來試試，如果讓車子只往一邊跑會發生什麼事XD："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1562306023408,
     "user": {
      "displayName": "Jiangsir jiang",
      "photoUrl": "https://lh3.googleusercontent.com/-IRgxJH2IITY/AAAAAAAAAAI/AAAAAAAAEPs/XqkMV_OB0lM/s64/photo.jpg",
      "userId": "14587696984096323544"
     },
     "user_tz": -480
    },
    "id": "xCjIrwlsyic9",
    "outputId": "870fab60-d80a-4019-a93b-2d4b70aaa110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "環境已達終止條件，總共獲得獎勵： 10.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPMkK5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "\n",
    "# 隨機從 action space 中挑一個動作，然後從頭到尾只做它XD\n",
    "action = env.action_space.sample() \n",
    "\n",
    "total_rewards = 0.0\n",
    "for t in range(50): # 至多行動 50 次\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_rewards += reward\n",
    "    \n",
    "    print(\"現在是第 {} 個時間點，獲得獎勵 {}，累計獎勵 {}，是否達終止條件？ {}\".format(t+1, reward, total_rewards, done))\n",
    "    env.render() # 顯示目前的環境狀態\n",
    "    \n",
    "    if done: # 若達終止條件，則結束執行\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"環境已達終止條件，總共獲得獎勵：\", total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnAhhSj2KToe"
   },
   "source": [
    "好的，如果一直讓滑車往左或右跑的話，一下子遊戲就停掉了，才獲得 10 分的獎勵而已，甚至比隨機選的結果還差！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EG5oaS4SGvCo"
   },
   "source": [
    "## 改進策略之 2：來個工人智慧\n",
    "\n",
    "很顯然，無腦的走法是沒有用的（廢話），為了讓 agent 不會走得太無腦，我們再來引進一個簡單的策略：如果棒子向左傾（角度 < 0），則平台小車向左移以維持平衡；否則往右移。\n",
    "\n",
    "我們來將上述策略寫成一個函式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf7AcA9bG7ur"
   },
   "outputs": [],
   "source": [
    "# 定義策略\n",
    "def choose_action(observation):\n",
    "    pos, v, ang, rot = observation # observation space 中的四個值\n",
    "    return 0 if ang < 0 else 1 # 柱子左傾則小車左移，否則右移 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClSkfGrRGyz0"
   },
   "source": [
    "定義完了策略之後，我們將 `choose_action()` 放入我們的程式中，這次我們一次來跑個 200 輪（跑一次完整的遊戲，英文術語稱為「episode」）試試看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkThG_tilOtv",
    "outputId": "add3bae1-b090-4a78-efbc-bdf0e7338b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 25 回合，共獲得獎勵：25.0\n",
      "本輪 46 回合，共獲得獎勵：46.0\n",
      "本輪 31 回合，共獲得獎勵：31.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 42 回合，共獲得獎勵：42.0\n",
      "本輪 49 回合，共獲得獎勵：49.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 31 回合，共獲得獎勵：31.0\n",
      "本輪 37 回合，共獲得獎勵：37.0\n",
      "本輪 40 回合，共獲得獎勵：40.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 31 回合，共獲得獎勵：31.0\n",
      "本輪 44 回合，共獲得獎勵：44.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 47 回合，共獲得獎勵：47.0\n",
      "本輪 48 回合，共獲得獎勵：48.0\n",
      "本輪 49 回合，共獲得獎勵：49.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 49 回合，共獲得獎勵：49.0\n",
      "本輪 36 回合，共獲得獎勵：36.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 44 回合，共獲得獎勵：44.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 34 回合，共獲得獎勵：34.0\n",
      "本輪 25 回合，共獲得獎勵：25.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 42 回合，共獲得獎勵：42.0\n",
      "本輪 36 回合，共獲得獎勵：36.0\n",
      "本輪 34 回合，共獲得獎勵：34.0\n",
      "本輪 34 回合，共獲得獎勵：34.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 50 回合，共獲得獎勵：50.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 25 回合，共獲得獎勵：25.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 25 回合，共獲得獎勵：25.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 47 回合，共獲得獎勵：47.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 26 回合，共獲得獎勵：26.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 40 回合，共獲得獎勵：40.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 40 回合，共獲得獎勵：40.0\n",
      "本輪 40 回合，共獲得獎勵：40.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 44 回合，共獲得獎勵：44.0\n",
      "本輪 40 回合，共獲得獎勵：40.0\n",
      "本輪 34 回合，共獲得獎勵：34.0\n",
      "本輪 32 回合，共獲得獎勵：32.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 47 回合，共獲得獎勵：47.0\n",
      "本輪 26 回合，共獲得獎勵：26.0\n",
      "本輪 34 回合，共獲得獎勵：34.0\n",
      "本輪 49 回合，共獲得獎勵：49.0\n",
      "本輪 36 回合，共獲得獎勵：36.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 43 回合，共獲得獎勵：43.0\n",
      "本輪 47 回合，共獲得獎勵：47.0\n",
      "本輪 40 回合，共獲得獎勵：40.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 25 回合，共獲得獎勵：25.0\n",
      "本輪 24 回合，共獲得獎勵：24.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 49 回合，共獲得獎勵：49.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 24 回合，共獲得獎勵：24.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 43 回合，共獲得獎勵：43.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 47 回合，共獲得獎勵：47.0\n",
      "本輪 46 回合，共獲得獎勵：46.0\n",
      "本輪 26 回合，共獲得獎勵：26.0\n",
      "本輪 36 回合，共獲得獎勵：36.0\n",
      "本輪 34 回合，共獲得獎勵：34.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 26 回合，共獲得獎勵：26.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 26 回合，共獲得獎勵：26.0\n",
      "本輪 48 回合，共獲得獎勵：48.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 49 回合，共獲得獎勵：49.0\n",
      "本輪 40 回合，共獲得獎勵：40.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 48 回合，共獲得獎勵：48.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 42 回合，共獲得獎勵：42.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 46 回合，共獲得獎勵：46.0\n",
      "本輪 46 回合，共獲得獎勵：46.0\n",
      "本輪 25 回合，共獲得獎勵：25.0\n",
      "本輪 36 回合，共獲得獎勵：36.0\n",
      "本輪 44 回合，共獲得獎勵：44.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 46 回合，共獲得獎勵：46.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 32 回合，共獲得獎勵：32.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 25 回合，共獲得獎勵：25.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 37 回合，共獲得獎勵：37.0\n",
      "本輪 36 回合，共獲得獎勵：36.0\n",
      "本輪 43 回合，共獲得獎勵：43.0\n",
      "本輪 34 回合，共獲得獎勵：34.0\n",
      "本輪 43 回合，共獲得獎勵：43.0\n",
      "本輪 40 回合，共獲得獎勵：40.0\n",
      "本輪 47 回合，共獲得獎勵：47.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 50 回合，共獲得獎勵：50.0\n",
      "本輪 42 回合，共獲得獎勵：42.0\n",
      "本輪 34 回合，共獲得獎勵：34.0\n",
      "本輪 48 回合，共獲得獎勵：48.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 25 回合，共獲得獎勵：25.0\n",
      "本輪 48 回合，共獲得獎勵：48.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 34 回合，共獲得獎勵：34.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 34 回合，共獲得獎勵：34.0\n",
      "本輪 45 回合，共獲得獎勵：45.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 40 回合，共獲得獎勵：40.0\n",
      "本輪 43 回合，共獲得獎勵：43.0\n",
      "本輪 48 回合，共獲得獎勵：48.0\n",
      "本輪 38 回合，共獲得獎勵：38.0\n",
      "本輪 42 回合，共獲得獎勵：42.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 49 回合，共獲得獎勵：49.0\n",
      "本輪 41 回合，共獲得獎勵：41.0\n",
      "本輪 26 回合，共獲得獎勵：26.0\n",
      "本輪 35 回合，共獲得獎勵：35.0\n",
      "本輪 42 回合，共獲得獎勵：42.0\n",
      "本輪 40 回合，共獲得獎勵：40.0\n",
      "本輪 31 回合，共獲得獎勵：31.0\n",
      "本輪 39 回合，共獲得獎勵：39.0\n",
      "本輪 25 回合，共獲得獎勵：25.0\n",
      "本輪 32 回合，共獲得獎勵：32.0\n",
      "平均獲得獎勵： 38.825301204819276\n",
      "最多的一次： 50.0\n",
      "最少的一次： 24.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# 來一次跑個 200 輪試試看\n",
    "# 由於跑 200 次有點久，從這開始我們就不顯示環境狀態囉，改為直接印出每一輪的結果\n",
    "total_rewards = []\n",
    "for i_episode in range(200): \n",
    "    observation = env.reset()\n",
    "    rewards = 0.0\n",
    "\n",
    "    for t in range(50): # 至多行動 50 次\n",
    "        observation, reward, done, info = env.step( choose_action(observation) )\n",
    "        rewards += reward\n",
    "        \n",
    "        if done: # 若達終止條件，則結束執行\n",
    "            print(\"本輪 {} 回合，共獲得獎勵：{}\".format(t+1, rewards) )\n",
    "            total_rewards.append(rewards)\n",
    "            break\n",
    "\n",
    "total_rewards = np.array(total_rewards)\n",
    "print(\"平均獲得獎勵：\", total_rewards.mean() )\n",
    "print(\"最多的一次：\", total_rewards.max() )\n",
    "print(\"最少的一次：\", total_rewards.min() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVLhntKclOt2"
   },
   "source": [
    "可以看到這樣簡單的策略，也比起之前略有改進囉！但好，還能更好！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hP0I7a6EEFPG"
   },
   "source": [
    "## 改進策略之 3：利用 Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UbUNrmtEOC0"
   },
   "source": [
    "接著我們就來利用課堂上學習過的 Q-Learning 來改進策略吧！\n",
    "\n",
    "### 回顧 Q-Learning 基本概念\n",
    "\n",
    "我們說增強式學習是基於「環境對於機器做出的動作所給予的回饋」學習的。在經過學習之後，機器會在不同的環境狀態中選擇最佳（能獲得最多獎勵）的行動，在 Q-Learning 中，我們把機器在某**狀態**下執行某**行動**的「*價值*」量化為 **Q (Quality) 值**，這個值是*愈高愈好*。而我們會把一個環境下的所有 Q 值存在一個 **Q-Table (Q 表)** 中。舉個例子，若今天某環境下有 3 種狀態，每個狀態下皆可執行 2 種行動，那麼 Q-Table 就會長得像這樣：\n",
    "\n",
    "|    Q-Table    | ( Action 1 ) | ( Action 2 ) |\n",
    "|---------------|--------------|--------------|\n",
    "| **(State 1)** | $Q(s_1,a_1)$ | $Q(s_1,a_2)$ |\n",
    "| **(State 2)** | $Q(s_2,a_1)$ | $Q(s_2,a_2)$ |\n",
    "| **(State 3)** | $Q(s_3,a_1)$ | $Q(s_3,a_2)$ |\n",
    "\n",
    "若假設機器位於 State 2 ($s_2$)，那麼機器就會選擇 State 2 那一列中 ($Q(s_2,a_1)$, $Q(s_2,a_2)$) Q 值最高的那個行動作為最佳行動。Q-Learning 演算法的核心就在於機器經過一次又一次的嘗試，Q-Table 中的 Q 值會不斷地被更新。我們在課堂中提過，Q 值會被兩種因素所影響（基於[貝爾曼方程](https://en.wikipedia.org/wiki/Bellman_equation)所設計）：\n",
    " * 選了這個行動後可以立即獲得的獎勵 ($R(s,a)$)\n",
    " * 移動到下一個狀態後可以獲得的最大 Q 值（下一個狀態中所有可能行動的 Q 值最大者）($\\max_{a}Q(s_{next},a)$)\n",
    "\n",
    "換句話說，Q 值能夠估計在某狀態下選擇某行動所產生的獎勵（故 Q 亦稱「價值函數 (Value function)」），可以寫成如下的公式：\n",
    "\n",
    "$$Q'(s,a) = R(s,a) + \\gamma \\max_{a}Q(s_{next},a)$$\n",
    "\n",
    "而 Q 值的更新方式有很多種，課堂上我們介紹了「**Temporal Difference, TD** (時值差異，又稱時間差分)」法，藉由定義一個 TD 值來代表**新舊 Q 值的差異**：\n",
    "\n",
    "\\begin{align}\n",
    "TD(s,a) & = Q'(s,a) - Q(s,a) \\\\\n",
    "        & = R(s,a) + \\gamma \\max_{a}Q(s_{next},a) - Q(s,a)\n",
    "\\end{align}\n",
    "\n",
    "然後將舊的 Q 值加上*加權過*的 TD 值就是更新後的 Q 值：\n",
    "\n",
    "\\begin{align}\n",
    "Q^{new}(s,a) & = Q(s,a) + \\alpha TD(s,a) \\\\\n",
    "             & = Q(s,a) + \\alpha [R(s,a) + \\gamma \\max_{a}Q(s_{next},a) - Q(s,a)] \n",
    "\\end{align}\n",
    "\n",
    "以上就是更新 Q 值的完整公式，其中：\n",
    " * $\\gamma$ (gamma) 稱為「**衰減係數 (Discount factor)**」(範圍：$0 \\le \\gamma \\le 1$)：用來控制*下個狀態的 Q 值對當前 Q 值的影響力*。這個值愈大，代表「未來決策的影響力愈大」，也就是將眼光放遠，注重長期利益；相反地，若這個值愈小，能馬上獲得的獎勵愈重要，某種程度上可視為「注重短期利益」。衰減係數並沒有公認要使用多少比較好，而是依照環境的性質做調整。\n",
    " * $\\alpha$ (alpha) 稱為「**學習速率 (Learning rate)**」(範圍：$0 \\lt \\alpha \\le 1$)：用來控制 *TD 值影響原 Q 值的程度*。這個值愈大代表傾向採用新的 Q 值來取代舊 Q 值，也就是變動幅度較大；愈小代表傾向保留舊 Q 值，也就是變動幅度較小。而這個值的大小也是依據使用的場景而有所不同，甚至有時候也要依據跑的輪數而有所變動。\n",
    "\n",
    "回顧完 Q-Learning 後，我們就開始著手設計程式吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fwxcFGnlOt6"
   },
   "source": [
    "### 建立 Q-Table\n",
    "\n",
    "首先我們先著手建立 Q-Table，要建立 Q-Table 時，需要知道**狀態與行動的數量**，我們可以使用前面提過的 `observation_space` 與 `action_space` 得知。\n",
    "\n",
    "另外還有一個問題，Q-Table 中的狀態數量是「有限」的，但是依照 wiki 上關於 CartPole-v0 的定義，其四個觀察值都是連續的，也就是會有「無限」個狀態。為了克服這個問題，我們需要將狀態有限化，方法是將依照觀察值的特性，分成數個*區段*作為狀態。假設我們定義某觀察值的上下界是 10 ~ 20，分成 4 個狀態，我們可以這樣分配：\n",
    "\n",
    "| 觀察值 | 狀態 |\n",
    "|-------|---------|\n",
    "| ~ 12.5 | 狀態 1 |\n",
    "| 12.5 ~ 15.0 | 狀態 2 |\n",
    "| 15.0 ~ 17.5 | 狀態 3 |\n",
    "| 17.5 ~ | 狀態 4 |\n",
    "\n",
    "狀態的數量與值的上下界可以根據*這個狀態是否重要*及*觀察值的分佈程度*而定。若觀察值在某個區間分佈非常密集，而在這個區間中微小的數值差異可能導致結果很不同，就可以考慮對這個區間細分多一點狀態。\n",
    "\n",
    "最後，我們要初始化 Q-Table，Q-Table 中一開始每個值都會是一個固定數字，通常及本範例中會定為 0.0；不過[也有學者指出](http://incompleteideas.net/book/first/ebook/node21.html)給定一個較大的值會有助於學習。\n",
    "\n",
    "以下是這一段的範例程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV2yyrqPlOt8"
   },
   "outputs": [],
   "source": [
    "## 建立 Q-Table ##\n",
    "\n",
    "env = gym.make('CartPole-v0') # 新創造一個 CartPole-v0' 環境\n",
    "\n",
    "# 環境中各個觀察值的狀態分配數量\n",
    "# 1 代表任何值皆表示同一 state，也就是這個觀察值其實不重要\n",
    "n_buckets = (1, 1, 6, 3) # CartPole-v0 中四個觀察值分別給予的狀態數量\n",
    "                         # **這些值同學們可以自由進行替換，這邊先代入編者們調校過的數值**\n",
    "\n",
    "# CartPole-v0 的 Action 數量 \n",
    "n_actions = env.action_space.n # 可以用 Discrete 的 n 這個屬性來取得 action 的數量\n",
    "\n",
    "# 取得各觀察值的上下界\n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "# 由於平台的移動速度及棒子末端速度原來的上下界是：負無限大～無限大，範圍過廣，故我們需手動設定合理的範圍取代之\n",
    "# **這些值同學們也可以自由進行替換，這邊先代入編者們調校過的數值**\n",
    "state_bounds[1] = [-0.5, 0.5] \n",
    "state_bounds[3] = [-math.radians(50), math.radians(50)]\n",
    "\n",
    "# 建立 Q-Table\n",
    "# 這裡我們用 np.zeros() 將內部的值都初始化為 0.0\n",
    "# 若要用其他值初始化，可以使用 np.full()，它比 np.zeros() 多加一個參數來指定初始值\n",
    "q_table = np.zeros(n_buckets + (n_actions,)) # 每個 state-action pair 存一個 Q 值\n",
    "                                             # 以我們的預設值來說這裡會產生一個形狀為 (1,1,6,3,2) 的五維陣列\n",
    "                                             # 也就是共 1*1*6*3 種「狀態組合」，每種狀態組合會有代表兩種行動的 2 個 Q 值\n",
    "                                             # 合計共 1*1*6*3*2 個 Q 值\n",
    "\n",
    "# 寫一個函數將觀察值轉為狀態組合\n",
    "# 我們會用這個狀態組合作為 Q-Table 的索引來找到正確的 Q 值\n",
    "def get_state(observation, n_buckets, state_bounds):\n",
    "    state = [0] * len(observation) \n",
    "    for i, s in enumerate(observation): # 每個觀察值有不同的分配\n",
    "        l, u = state_bounds[i][0], state_bounds[i][1] # 每個觀察值值的範圍上下限\n",
    "        if s <= l: # 低於下限，分配為值最低的那個狀態\n",
    "            state[i] = 0\n",
    "        elif s >= u: # 高於上限，分配為值最高的那個狀態\n",
    "            state[i] = n_buckets[i] - 1\n",
    "        else: # 範圍內，依比例分配\n",
    "            state[i] = int(((s - l) / (u - l)) * n_buckets[i])\n",
    "    return tuple(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yNg3pAjlOuA"
   },
   "source": [
    "### 行動挑選策略\n",
    "\n",
    "還記得我們在課堂上有提過，如果我們一直只循著 Q 值最高的路（貪婪法）走，就可能會陷入「只會一直走同一條路（老是使用相同的策略）」的問題嗎？因此我們引進了貪婪法的改良版：「**ε-貪婪 (ε-greedy, epsilon-greedy)**」來**讓機器在一定的機率下（不參考 Q 值）隨機選擇行動**，讓機器擁有更多可能性，說不定會發現更好的策略。\n",
    "\n",
    "所以我們定義一個函式來實作 ε-greedy："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxOfJ6KHlOuA"
   },
   "outputs": [],
   "source": [
    "def choose_action(state, q_table, action_space, epsilon):\n",
    "    if np.random.random_sample() < epsilon: # 有 ε 的機率會選擇隨機行動\n",
    "        return action_space.sample() \n",
    "    else: # 其他情況下參考 Q-Table 選擇行動\n",
    "        return np.argmax(q_table[state]) # 在 Q-Table 中目前的狀態組合下，選擇擁有最大 Q 值的行動"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QzAgv9szlOuE"
   },
   "source": [
    "### 定義 Hyperparameters (超參數)\n",
    "\n",
    "然後來定義 **hyperparameters (超參數)**，hyperparameters 是機器學習中可供使用者自行調整的參數（例如 Q-Learning 的衰減係數與學習速率）；相對於由機器在學習過程中自行調整的 parameters（參數，例如 Q-Learning 中的 Q 值）。\n",
    "\n",
    "上面提過，衰減係數與學習速率的大小均依照應用場景而有所不同，在此為了方便 Q 值收斂，我們將 ε 和學習速率設定成會隨著時間 (episode) 遞減，也就是我們從大膽亂走，到愈來愈相信已經學到的經驗："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6Zzj8Xezrgg"
   },
   "outputs": [],
   "source": [
    "# **相同地，這些數值同學們也可以自由進行替換，這邊先代入編者們調校過的數值**\n",
    "get_epsilon = lambda i: max(0.01, min(1.0, 1.0 - math.log10((i+1)/25))) # epsilon-greedy，隨時間遞減\n",
    "get_lr      = lambda i: max(0.01, min(0.5, 1.0 - math.log10((i+1)/25))) # 學習速率，隨時間遞減 \n",
    "get_gamma   = lambda i: 0.99 # 衰減係數我們設定成不隨時間改變的固定值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XEoig3KEfSv"
   },
   "source": [
    "### 開始學習\n",
    "\n",
    "最後是學習，也就是 Q-Learning 演算法主程式的部份："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3575
    },
    "colab_type": "code",
    "id": "NqTcvCqJEa0T",
    "outputId": "8ff5db3c-1f47-4c46-cb81-7b4d87996136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  14 回合，共獲得獎勵： 14.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  34 回合，共獲得獎勵： 34.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  16 回合，共獲得獎勵： 16.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  22 回合，共獲得獎勵： 22.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  23 回合，共獲得獎勵： 23.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  21 回合，共獲得獎勵： 21.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  41 回合，共獲得獎勵： 41.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  13 回合，共獲得獎勵： 13.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  48 回合，共獲得獎勵： 48.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  14 回合，共獲得獎勵： 14.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  15 回合，共獲得獎勵： 15.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  26 回合，共獲得獎勵： 26.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  13 回合，共獲得獎勵： 13.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  25 回合，共獲得獎勵： 25.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  30 回合，共獲得獎勵： 30.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  12 回合，共獲得獎勵： 12.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  66 回合，共獲得獎勵： 66.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  14 回合，共獲得獎勵： 14.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  27 回合，共獲得獎勵： 27.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  20 回合，共獲得獎勵： 20.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  13 回合，共獲得獎勵： 13.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  21 回合，共獲得獎勵： 21.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  19 回合，共獲得獎勵： 19.0 (epsilon=1.0000 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  24 回合，共獲得獎勵： 24.0 (epsilon=0.9830 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  21 回合，共獲得獎勵： 21.0 (epsilon=0.9666 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  52 回合，共獲得獎勵： 52.0 (epsilon=0.9508 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  28 回合，共獲得獎勵： 28.0 (epsilon=0.9355 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  18 回合，共獲得獎勵： 18.0 (epsilon=0.9208 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  12 回合，共獲得獎勵： 12.0 (epsilon=0.9066 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  13 回合，共獲得獎勵： 13.0 (epsilon=0.8928 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  12 回合，共獲得獎勵： 12.0 (epsilon=0.8794 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  30 回合，共獲得獎勵： 30.0 (epsilon=0.8665 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  12 回合，共獲得獎勵： 12.0 (epsilon=0.8539 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  10 回合，共獲得獎勵： 10.0 (epsilon=0.8416 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  28 回合，共獲得獎勵： 28.0 (epsilon=0.8297 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  12 回合，共獲得獎勵： 12.0 (epsilon=0.8182 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  12 回合，共獲得獎勵： 12.0 (epsilon=0.8069 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=0.7959 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  32 回合，共獲得獎勵： 32.0 (epsilon=0.7852 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  10 回合，共獲得獎勵： 10.0 (epsilon=0.7747 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=0.7645 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  12 回合，共獲得獎勵： 12.0 (epsilon=0.7545 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=0.7447 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  15 回合，共獲得獎勵： 15.0 (epsilon=0.7352 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  34 回合，共獲得獎勵： 34.0 (epsilon=0.7258 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  20 回合，共獲得獎勵： 20.0 (epsilon=0.7167 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  36 回合，共獲得獎勵： 36.0 (epsilon=0.7077 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=0.6990 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  16 回合，共獲得獎勵： 16.0 (epsilon=0.6904 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  28 回合，共獲得獎勵： 28.0 (epsilon=0.6819 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  10 回合，共獲得獎勵： 10.0 (epsilon=0.6737 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=0.6655 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  16 回合，共獲得獎勵： 16.0 (epsilon=0.6576 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  39 回合，共獲得獎勵： 39.0 (epsilon=0.6498 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  28 回合，共獲得獎勵： 28.0 (epsilon=0.6421 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=0.6345 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  23 回合，共獲得獎勵： 23.0 (epsilon=0.6271 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  53 回合，共獲得獎勵： 53.0 (epsilon=0.6198 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  20 回合，共獲得獎勵： 20.0 (epsilon=0.6126 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  30 回合，共獲得獎勵： 30.0 (epsilon=0.6055 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  32 回合，共獲得獎勵： 32.0 (epsilon=0.5986 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪   8 回合，共獲得獎勵：  8.0 (epsilon=0.5918 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  94 回合，共獲得獎勵： 94.0 (epsilon=0.5850 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  47 回合，共獲得獎勵： 47.0 (epsilon=0.5784 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  15 回合，共獲得獎勵： 15.0 (epsilon=0.5719 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  22 回合，共獲得獎勵： 22.0 (epsilon=0.5654 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  10 回合，共獲得獎勵： 10.0 (epsilon=0.5591 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  29 回合，共獲得獎勵： 29.0 (epsilon=0.5528 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  13 回合，共獲得獎勵： 13.0 (epsilon=0.5467 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  23 回合，共獲得獎勵： 23.0 (epsilon=0.5406 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  16 回合，共獲得獎勵： 16.0 (epsilon=0.5346 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  12 回合，共獲得獎勵： 12.0 (epsilon=0.5287 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  36 回合，共獲得獎勵： 36.0 (epsilon=0.5229 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  39 回合，共獲得獎勵： 39.0 (epsilon=0.5171 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.5114 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  28 回合，共獲得獎勵： 28.0 (epsilon=0.5058 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  17 回合，共獲得獎勵： 17.0 (epsilon=0.5003 learning_rate=0.5000 discount_factor=0.9900)\n",
      "本輪  32 回合，共獲得獎勵： 32.0 (epsilon=0.4949 learning_rate=0.4949 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=0.4895 learning_rate=0.4895 discount_factor=0.9900)\n",
      "本輪  26 回合，共獲得獎勵： 26.0 (epsilon=0.4841 learning_rate=0.4841 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=0.4789 learning_rate=0.4789 discount_factor=0.9900)\n",
      "本輪  37 回合，共獲得獎勵： 37.0 (epsilon=0.4737 learning_rate=0.4737 discount_factor=0.9900)\n",
      "本輪  19 回合，共獲得獎勵： 19.0 (epsilon=0.4685 learning_rate=0.4685 discount_factor=0.9900)\n",
      "本輪  52 回合，共獲得獎勵： 52.0 (epsilon=0.4634 learning_rate=0.4634 discount_factor=0.9900)\n",
      "本輪  27 回合，共獲得獎勵： 27.0 (epsilon=0.4584 learning_rate=0.4584 discount_factor=0.9900)\n",
      "本輪  15 回合，共獲得獎勵： 15.0 (epsilon=0.4535 learning_rate=0.4535 discount_factor=0.9900)\n",
      "本輪  16 回合，共獲得獎勵： 16.0 (epsilon=0.4486 learning_rate=0.4486 discount_factor=0.9900)\n",
      "本輪  14 回合，共獲得獎勵： 14.0 (epsilon=0.4437 learning_rate=0.4437 discount_factor=0.9900)\n",
      "本輪 116 回合，共獲得獎勵：116.0 (epsilon=0.4389 learning_rate=0.4389 discount_factor=0.9900)\n",
      "本輪  35 回合，共獲得獎勵： 35.0 (epsilon=0.4342 learning_rate=0.4342 discount_factor=0.9900)\n",
      "本輪  25 回合，共獲得獎勵： 25.0 (epsilon=0.4295 learning_rate=0.4295 discount_factor=0.9900)\n",
      "本輪  21 回合，共獲得獎勵： 21.0 (epsilon=0.4248 learning_rate=0.4248 discount_factor=0.9900)\n",
      "本輪  13 回合，共獲得獎勵： 13.0 (epsilon=0.4202 learning_rate=0.4202 discount_factor=0.9900)\n",
      "本輪  19 回合，共獲得獎勵： 19.0 (epsilon=0.4157 learning_rate=0.4157 discount_factor=0.9900)\n",
      "本輪  61 回合，共獲得獎勵： 61.0 (epsilon=0.4112 learning_rate=0.4112 discount_factor=0.9900)\n",
      "本輪  12 回合，共獲得獎勵： 12.0 (epsilon=0.4067 learning_rate=0.4067 discount_factor=0.9900)\n",
      "本輪  25 回合，共獲得獎勵： 25.0 (epsilon=0.4023 learning_rate=0.4023 discount_factor=0.9900)\n",
      "本輪  23 回合，共獲得獎勵： 23.0 (epsilon=0.3979 learning_rate=0.3979 discount_factor=0.9900)\n",
      "本輪  26 回合，共獲得獎勵： 26.0 (epsilon=0.3936 learning_rate=0.3936 discount_factor=0.9900)\n",
      "本輪  32 回合，共獲得獎勵： 32.0 (epsilon=0.3893 learning_rate=0.3893 discount_factor=0.9900)\n",
      "本輪  10 回合，共獲得獎勵： 10.0 (epsilon=0.3851 learning_rate=0.3851 discount_factor=0.9900)\n",
      "本輪  49 回合，共獲得獎勵： 49.0 (epsilon=0.3809 learning_rate=0.3809 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=0.3768 learning_rate=0.3768 discount_factor=0.9900)\n",
      "本輪  14 回合，共獲得獎勵： 14.0 (epsilon=0.3726 learning_rate=0.3726 discount_factor=0.9900)\n",
      "本輪   9 回合，共獲得獎勵：  9.0 (epsilon=0.3686 learning_rate=0.3686 discount_factor=0.9900)\n",
      "本輪  14 回合，共獲得獎勵： 14.0 (epsilon=0.3645 learning_rate=0.3645 discount_factor=0.9900)\n",
      "本輪  68 回合，共獲得獎勵： 68.0 (epsilon=0.3605 learning_rate=0.3605 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3565 learning_rate=0.3565 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3526 learning_rate=0.3526 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3487 learning_rate=0.3487 discount_factor=0.9900)\n",
      "本輪  52 回合，共獲得獎勵： 52.0 (epsilon=0.3449 learning_rate=0.3449 discount_factor=0.9900)\n",
      "本輪  52 回合，共獲得獎勵： 52.0 (epsilon=0.3410 learning_rate=0.3410 discount_factor=0.9900)\n",
      "本輪  37 回合，共獲得獎勵： 37.0 (epsilon=0.3372 learning_rate=0.3372 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3335 learning_rate=0.3335 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3298 learning_rate=0.3298 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3261 learning_rate=0.3261 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3224 learning_rate=0.3224 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3188 learning_rate=0.3188 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3152 learning_rate=0.3152 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3116 learning_rate=0.3116 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.3080 learning_rate=0.3080 discount_factor=0.9900)\n",
      "本輪  37 回合，共獲得獎勵： 37.0 (epsilon=0.3045 learning_rate=0.3045 discount_factor=0.9900)\n",
      "本輪   8 回合，共獲得獎勵：  8.0 (epsilon=0.3010 learning_rate=0.3010 discount_factor=0.9900)\n",
      "本輪  64 回合，共獲得獎勵： 64.0 (epsilon=0.2976 learning_rate=0.2976 discount_factor=0.9900)\n",
      "本輪   9 回合，共獲得獎勵：  9.0 (epsilon=0.2941 learning_rate=0.2941 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2907 learning_rate=0.2907 discount_factor=0.9900)\n",
      "本輪  83 回合，共獲得獎勵： 83.0 (epsilon=0.2874 learning_rate=0.2874 discount_factor=0.9900)\n",
      "本輪  17 回合，共獲得獎勵： 17.0 (epsilon=0.2840 learning_rate=0.2840 discount_factor=0.9900)\n",
      "本輪  77 回合，共獲得獎勵： 77.0 (epsilon=0.2807 learning_rate=0.2807 discount_factor=0.9900)\n",
      "本輪  14 回合，共獲得獎勵： 14.0 (epsilon=0.2774 learning_rate=0.2774 discount_factor=0.9900)\n",
      "本輪  36 回合，共獲得獎勵： 36.0 (epsilon=0.2741 learning_rate=0.2741 discount_factor=0.9900)\n",
      "本輪  15 回合，共獲得獎勵： 15.0 (epsilon=0.2708 learning_rate=0.2708 discount_factor=0.9900)\n",
      "本輪 144 回合，共獲得獎勵：144.0 (epsilon=0.2676 learning_rate=0.2676 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2644 learning_rate=0.2644 discount_factor=0.9900)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本輪 126 回合，共獲得獎勵：126.0 (epsilon=0.2612 learning_rate=0.2612 discount_factor=0.9900)\n",
      "本輪  19 回合，共獲得獎勵： 19.0 (epsilon=0.2581 learning_rate=0.2581 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2549 learning_rate=0.2549 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2518 learning_rate=0.2518 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2487 learning_rate=0.2487 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2457 learning_rate=0.2457 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2426 learning_rate=0.2426 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2396 learning_rate=0.2396 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2366 learning_rate=0.2366 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2336 learning_rate=0.2336 discount_factor=0.9900)\n",
      "本輪 148 回合，共獲得獎勵：148.0 (epsilon=0.2306 learning_rate=0.2306 discount_factor=0.9900)\n",
      "本輪   9 回合，共獲得獎勵：  9.0 (epsilon=0.2277 learning_rate=0.2277 discount_factor=0.9900)\n",
      "本輪  25 回合，共獲得獎勵： 25.0 (epsilon=0.2248 learning_rate=0.2248 discount_factor=0.9900)\n",
      "本輪  33 回合，共獲得獎勵： 33.0 (epsilon=0.2218 learning_rate=0.2218 discount_factor=0.9900)\n",
      "本輪  12 回合，共獲得獎勵： 12.0 (epsilon=0.2190 learning_rate=0.2190 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2161 learning_rate=0.2161 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2132 learning_rate=0.2132 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2104 learning_rate=0.2104 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2076 learning_rate=0.2076 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2048 learning_rate=0.2048 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.2020 learning_rate=0.2020 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1993 learning_rate=0.1993 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1965 learning_rate=0.1965 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1938 learning_rate=0.1938 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1911 learning_rate=0.1911 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1884 learning_rate=0.1884 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1858 learning_rate=0.1858 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1831 learning_rate=0.1831 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1805 learning_rate=0.1805 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1778 learning_rate=0.1778 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1752 learning_rate=0.1752 discount_factor=0.9900)\n",
      "本輪  23 回合，共獲得獎勵： 23.0 (epsilon=0.1726 learning_rate=0.1726 discount_factor=0.9900)\n",
      "本輪  27 回合，共獲得獎勵： 27.0 (epsilon=0.1701 learning_rate=0.1701 discount_factor=0.9900)\n",
      "本輪  10 回合，共獲得獎勵： 10.0 (epsilon=0.1675 learning_rate=0.1675 discount_factor=0.9900)\n",
      "本輪  13 回合，共獲得獎勵： 13.0 (epsilon=0.1649 learning_rate=0.1649 discount_factor=0.9900)\n",
      "本輪  24 回合，共獲得獎勵： 24.0 (epsilon=0.1624 learning_rate=0.1624 discount_factor=0.9900)\n",
      "本輪  16 回合，共獲得獎勵： 16.0 (epsilon=0.1599 learning_rate=0.1599 discount_factor=0.9900)\n",
      "本輪  26 回合，共獲得獎勵： 26.0 (epsilon=0.1574 learning_rate=0.1574 discount_factor=0.9900)\n",
      "本輪  40 回合，共獲得獎勵： 40.0 (epsilon=0.1549 learning_rate=0.1549 discount_factor=0.9900)\n",
      "本輪  11 回合，共獲得獎勵： 11.0 (epsilon=0.1524 learning_rate=0.1524 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1500 learning_rate=0.1500 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1475 learning_rate=0.1475 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1451 learning_rate=0.1451 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1427 learning_rate=0.1427 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1403 learning_rate=0.1403 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1379 learning_rate=0.1379 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1355 learning_rate=0.1355 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1331 learning_rate=0.1331 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1308 learning_rate=0.1308 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1284 learning_rate=0.1284 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1261 learning_rate=0.1261 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1238 learning_rate=0.1238 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1215 learning_rate=0.1215 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1192 learning_rate=0.1192 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1169 learning_rate=0.1169 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1146 learning_rate=0.1146 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1124 learning_rate=0.1124 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1101 learning_rate=0.1101 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1079 learning_rate=0.1079 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1057 learning_rate=0.1057 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1035 learning_rate=0.1035 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.1013 learning_rate=0.1013 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.0991 learning_rate=0.0991 discount_factor=0.9900)\n",
      "本輪 200 回合，共獲得獎勵：200.0 (epsilon=0.0969 learning_rate=0.0969 discount_factor=0.9900)\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning 演算法\n",
    "for i_episode in range(200): # 跑個 200 輪\n",
    "    epsilon = get_epsilon(i_episode) # 計算這一輪的 epsilon-greedy 機率\n",
    "    lr      = get_lr(i_episode) # 計算這一輪的學習速率\n",
    "    gamma   = get_gamma(i_episode) # 計算這一輪的衰減係數\n",
    "\n",
    "    observation = env.reset() # 每一輪開始都要重設環境狀態\n",
    "    rewards = 0.0 # 還有重設累計獎勵\n",
    "    state = get_state(observation, n_buckets, state_bounds) # 計算最初環境是哪個狀態組合\n",
    "    for t in range(250):\n",
    "        action = choose_action(state, q_table, env.action_space, epsilon) # 根據目前的狀態組合選擇行動\n",
    "        observation, reward, done, info = env.step(action) # 實際行動，然後獲得該行動的獎勵及行動後環境的觀察值\n",
    "        next_state = get_state(observation, n_buckets, state_bounds) # 將觀察值轉為狀態組合\n",
    "        rewards += reward\n",
    "\n",
    "        # 更新 Q-Table\n",
    "        q_next_max = np.amax(q_table[next_state]) # 移動到下一個狀態後可以獲得的最大 Q 值'\n",
    "        \n",
    "        q_table[state + (action,)] += lr * (reward + gamma * q_next_max - q_table[state + (action,)]) # 就是「那個公式」\n",
    "\n",
    "        # 前進下一 state \n",
    "        state = next_state\n",
    "\n",
    "        if done: # 若達終止條件，則結束執行\n",
    "            print(\"本輪 {:3d} 回合，共獲得獎勵：{:5.1f} (epsilon={:.4f} learning_rate={:.4f} discount_factor={:.4f})\"\\\n",
    "                  .format(t+1, rewards, epsilon, lr, gamma) )\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZPekPl5MCIL"
   },
   "source": [
    "可以看到在訓練後期，我們的機器已經學會如何最大化自己的獎勵，也就是維持住小車上的棒子了。\n",
    "\n",
    "以上就是 Q-Learning 的程式，最後仍提醒一下同學，以上的許多參數，包括如何分配狀態（哪些觀察值是重要的）、觀察值的上下界、Q-Table 的初始值、學習速率、衰減係數、epsilon-greedy 機率等，都是需要多試幾次來反覆調整，來達到最好結果的。這邊為了展示給大家看，所以編者們就放上了前人調校過的參數。同學們也可以自己試試看，使用不同的值來玩玩看唷！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfQooH6MPait"
   },
   "source": [
    "## 同場加映：顯示平台小車如何利用 Q-Table 來保持平衡\n",
    "\n",
    "既然都求出 Q-Table了，那我們就寫一個程式畫出滑車如何利用 Q-Table 來保持平衡吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9riuZbu0FRoz",
    "outputId": "87b00fd8-3c4e-4bb0-a421-ac3296abbf1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "環境已達終止條件，總共獲得獎勵： 200.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# 這裡我們不取得那些超參數，因為不需要更新 Q-Table\n",
    "total_rewards = 0.0\n",
    "observation = env.reset()\n",
    "for t in range(250):\n",
    "    state = get_state(observation, n_buckets, state_bounds) # 將觀察值轉為計算狀態組合\n",
    "    action = choose_action(state, q_table, env.action_space, 0.0) # 根據 Q-Table 選擇行動\n",
    "                                                                  # 這裡將 epsilon 機率設為 0 防止亂走\n",
    "    observation, reward, done, info = env.step(action) # 執行行動\n",
    "    total_rewards += reward\n",
    "\n",
    "    print(\"第 {} 個時間點，已選擇行動 {}，目前累計獎勵：{}\".format(t+1, action, total_rewards))\n",
    "    env.render() # 顯示目前的環境狀態\n",
    "    \n",
    "    if done: # 若達終止條件，則結束執行\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"環境已達終止條件，總共獲得獎勵：\", total_rewards)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_reinforcement_learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
